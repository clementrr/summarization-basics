{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US President Donald Trump is breaking precedent once again by not meeting with America's Nobel prize winners. The eight US laureates have not been scheduled to see Mr Trump ahead of their December trip to Sweden to receive their awards. However, not all of them are disappointed at the prospect of missing out on the Oval office greeting. Joachim Frank, awarded the Nobel in chemistry for work in microscopy, told Stat News that he “will not put my foot into the White House as long as Trump, Pence...occupy it.”\n",
      "\n",
      "He also included House Speaker Paul Ryan on that list in case he ends up in the White House as a result of the “possible succession of impeachments\".\n"
     ]
    }
   ],
   "source": [
    "import sys, glob, nltk\n",
    "\n",
    "summary_length = 90\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [t.lower() for t in tokens if t not in stopwords]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "def compute_probs(cluster):\n",
    "    word_probs = {}\n",
    "    token_count = 0 \n",
    "    \n",
    "    for file_path in cluster:\n",
    "        with open(file_path) as f:\n",
    "            tokens = tokenize_sentence(f.read().decode(\"utf-8\"))\n",
    "            token_count += len(tokens)\n",
    "        for t in tokens:\n",
    "            if t not in word_probs:\n",
    "                word_probs[t] = 1.\n",
    "            else:\n",
    "                word_probs[t] += 1.\n",
    "                \n",
    "    return {k: (v / token_count) for k, v in word_probs.items()}\n",
    "    \n",
    "def extract_sentences(cluster):\n",
    "    sentences = []\n",
    "    \n",
    "    for file_path in cluster:\n",
    "        with open(file_path) as f:\n",
    "            sentences += nltk.sent_tokenize(f.read().decode(\"utf-8\"))\n",
    "            \n",
    "    return sentences\n",
    "        \n",
    "def score_sentence(sentence, word_probs):\n",
    "    score = 0.\n",
    "    token_count = 0\n",
    "    tokens = tokenize_sentence(sentence)\n",
    "    \n",
    "    for t in tokens:\n",
    "        score += word_probs[t]\n",
    "        token_count += 1\n",
    "        \n",
    "    return score / token_count\n",
    "\n",
    "def best_sentence(sentences, word_probs, non_redundancy):\n",
    "    best_sentence = sentences[0]\n",
    "    max_score = score_sentence(best_sentence, word_probs)\n",
    "    \n",
    "    for sent in sentences:\n",
    "        score = score_sentence(sent, word_probs)\n",
    "        if score > max_score:\n",
    "            best_sentence = sent\n",
    "            max_score = score\n",
    "    \n",
    "    if non_redundancy:\n",
    "        tokens = tokenize_sentence(best_sentence)\n",
    "        for t in tokens:\n",
    "            word_probs[t] = word_probs[t] ** 2\n",
    "        \n",
    "    return best_sentence\n",
    "            \n",
    "def sum_basic(cluster, non_redundancy):\n",
    "    cluster = glob.glob(cluster)\n",
    "    word_probs = compute_probs(cluster)\n",
    "    sentences = extract_sentences(cluster)\n",
    "\n",
    "    summary = []\n",
    "    word_count = 0\n",
    "    while word_count < summary_length:\n",
    "        sent = best_sentence(sentences, word_probs, non_redundancy)\n",
    "        summary.append(sent)\n",
    "        word_count += len(nltk.word_tokenize(sent))\n",
    "        sentences.remove(sent)\n",
    "        \n",
    "    return \" \".join(summary)\n",
    "\n",
    "def leading_baseline(cluster):\n",
    "    cluster = glob.glob(cluster)\n",
    "    sentences = extract_sentences(cluster)\n",
    "\n",
    "    summary = []\n",
    "    word_count = 0\n",
    "    while word_count < summary_length:\n",
    "        sent = sentences[0]\n",
    "        summary.append(sent)\n",
    "        word_count += len(nltk.word_tokenize(sent))\n",
    "        sentences.remove(sent)\n",
    "        \n",
    "    return \" \".join(summary)\n",
    "    \n",
    "method = \"leading\"\n",
    "cluster = \"docs/doc1-*.txt\"\n",
    "\n",
    "if method == \"orig\":\n",
    "    print sum_basic(cluster, True)\n",
    "elif method == \"simplified\":\n",
    "    print sum_basic(cluster, False)\n",
    "elif method == \"leading\":\n",
    "    print leading_baseline(cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
